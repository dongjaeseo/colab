{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "dacon004_third_128.ipynb",
      "private_outputs": true,
      "provenance": [],
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyPilF+brzeefLgMewIfF7Zt"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "dVxNOr1juRIN"
      },
      "source": [
        "from google.colab import drive \r\n",
        "drive.mount('/content/drive')\r\n",
        "\r\n",
        "import numpy as np\r\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\r\n",
        "from tensorflow.keras.models import Model, Sequential\r\n",
        "from tensorflow.keras.layers import Dense, Conv2D, MaxPooling2D, Dropout, BatchNormalization, Flatten\r\n",
        "from sklearn.model_selection import train_test_split\r\n",
        "from tensorflow.keras.callbacks import ReduceLROnPlateau, EarlyStopping, ModelCheckpoint\r\n",
        "import string\r\n",
        "import pandas as pd\r\n",
        "import cv2\r\n",
        "\r\n",
        "def mymodel():\r\n",
        "    model = Sequential()\r\n",
        "\r\n",
        "    model.add(Conv2D(16,(3,3),activation='relu',input_shape=(128,128,1),padding='same'))\r\n",
        "    model.add(BatchNormalization())\r\n",
        "    model.add(Dropout(0.3))\r\n",
        "    model.add(Conv2D(64,(5,5),activation='relu',padding = 'same'))\r\n",
        "    model.add(BatchNormalization())\r\n",
        "    model.add(MaxPooling2D(3))\r\n",
        "\r\n",
        "    model.add(Flatten())\r\n",
        "\r\n",
        "    model.add(Dense(256,activation='relu'))\r\n",
        "    model.add(BatchNormalization())\r\n",
        "    model.add(Dense(128,activation='relu'))\r\n",
        "    model.add(BatchNormalization())\r\n",
        "    model.add(Dropout(0.3))\r\n",
        "    model.add(Dense(64,activation='relu'))\r\n",
        "    model.add(BatchNormalization())\r\n",
        "    model.add(Dropout(0.3))\r\n",
        "\r\n",
        "    model.add(Dense(1,activation='sigmoid'))\r\n",
        "    return model\r\n",
        "    \r\n",
        "#1. 데이터\r\n",
        "x_train = np.load('/content/drive/MyDrive/dacon/npy/x_train_128.npy')\r\n",
        "x_test = np.load('/content/drive/MyDrive/dacon/npy/x_test_128.npy')\r\n",
        "answer = pd.read_csv('/content/drive/MyDrive/dacon/dirty_mnist_answer.csv')\r\n",
        "sub = pd.read_csv('/content/drive/MyDrive/dacon/sample_submission.csv', header = 0)\r\n",
        "\r\n",
        "# print(x_train.shape) (50000, 128, 128, 1)\r\n",
        "# print(np.max(x_train[0])) # 1.0\r\n",
        "\r\n",
        "alphabets = string.ascii_lowercase\r\n",
        "alphabets = list(alphabets)\r\n",
        "\r\n",
        "val = []\r\n",
        "acc = []\r\n",
        "for alphabet in alphabets:\r\n",
        "    y_train = answer.loc[:,alphabet].to_numpy()\r\n",
        "\r\n",
        "    #2. 모델\r\n",
        "    model = mymodel()\r\n",
        "\r\n",
        "    #3. 컴파일 훈련\r\n",
        "    es = EarlyStopping(patience = 20)\r\n",
        "    lr = ReduceLROnPlateau(patience = 7, factor = 0.25, verbose = 1)\r\n",
        "    cp = ModelCheckpoint(f'/content/drive/MyDrive/dacon/mcp/{alphabet}.hdf5')\r\n",
        "    model.compile(loss = 'binary_crossentropy', optimizer= 'adam', metrics = ['acc'])\r\n",
        "    history = model.fit(x_train, y_train, epochs = 10000, batch_size = 128, validation_split = 0.2, callbacks = [es,cp,lr])\r\n",
        "    val.append(history.history['val_loss'][-21])\r\n",
        "    acc.append(history.history['val_acc'][-21])\r\n",
        "\r\n",
        "print('loss : ', val)\r\n",
        "print('acc : ', acc)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}