{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "authorship_tag": "ABX9TyOC1LQLqMfV3Bpw6qY+dhHi",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dongjaeseo/colab/blob/main/backpropagation3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VWI08qkKV9IP"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from matplotlib import pyplot as plt"
      ],
      "metadata": {
        "id": "WaLOA4gUWRFC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_data = np.loadtxt('/content/drive/MyDrive/data/backprop/training.dat', unpack = True)\n",
        "test_data = np.loadtxt('/content/drive/MyDrive/data/backprop/testing.dat', unpack = True)"
      ],
      "metadata": {
        "id": "h-QxXca3WSQD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_label = []\n",
        "test_label = []\n",
        "for i in range(3):\n",
        "    for j in range(25):\n",
        "        label = np.zeros(3)\n",
        "        label[i] = 1\n",
        "        train_label.append(label)\n",
        "        test_label.append(label)\n",
        "    \n",
        "train_label = np.array(train_label).T\n",
        "test_label = np.array(test_label).T"
      ],
      "metadata": {
        "id": "V8jFcl9JWVLB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define Activation Functions\n",
        "def sigmoid(x):\n",
        "    return 1/(1+np.exp(-x))\n",
        "\n",
        "def relu(x):\n",
        "    return np.maximum(0,x)\n",
        "\n",
        "def softmax(x):\n",
        "    return np.exp(x)/np.sum(np.exp(x),axis=0)"
      ],
      "metadata": {
        "id": "KfEynpbhWaer"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nx, nh1, nh2, ny = 4, 3, 3, 3\n",
        "U = np.random.randn(nx , nh1)*2\n",
        "V = np.random.randn(nh1, nh2)*2\n",
        "W = np.random.randn(nh2, ny)*2\n",
        "learning_rate = 1e-1"
      ],
      "metadata": {
        "id": "2GHOWMs3mKDa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input = np.zeros(nx)\n",
        "\n",
        "h1_out, h1_deriv = np.zeros(nh1), np.zeros(nh1) # 순전파시 계산 - 은닉계층 1\n",
        "h1_delta = np.zeros(nh1)                        # 역전파시 계산\n",
        "\n",
        "h2_out, h2_deriv = np.zeros(nh2), np.zeros(nh2) # 순전파시 계산 - 은닉계층 2\n",
        "h2_delta = np.zeros(nh2)                        # 역전파시 계산\n",
        "\n",
        "y_out, y_deriv = np.zeros(ny), np.zeros(ny)     # 순전파시 계산 - 출력계층\n",
        "y_delta = np.zeros(ny)                          # 역전파시 계산"
      ],
      "metadata": {
        "id": "zuerxb1DmTWn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def model(X, Y, learning_rate = 0.1, num_iterations = 20, activation = sigmoid):\n",
        "    layer_dims = [X.shape[0], 4, 4, 3]  \n",
        "    grads = {}\n",
        "    train_costs = []\n",
        "    test_costs = []\n",
        "\n",
        "    # Initialize weights - Glorot Initialization\n",
        "    parameters = {}\n",
        "    for i in range(1, len(layer_dims)):\n",
        "        m = np.sqrt(6/(layer_dims[i]+layer_dims[i-1]))\n",
        "        parameters['W' + str(i)] = np.random.uniform(-m, m, size = (layer_dims[i], layer_dims[i-1]))\n",
        "    \n",
        "    for i in range(0, num_iterations):       \n",
        "        # Forward propagation\n",
        "        W1 = parameters[\"W1\"]\n",
        "        W2 = parameters[\"W2\"]\n",
        "        W3 = parameters[\"W3\"]\n",
        "\n",
        "        # Forward propagation\n",
        "        Z1 = np.dot(W1, X)\n",
        "        A1 = activation(Z1)\n",
        "        Z2 = np.dot(W2, A1)\n",
        "        A2 = activation(Z2)\n",
        "        Z3 = np.dot(W3, A2)\n",
        "        Y_pred = softmax(Z3)\n",
        "\n",
        "        # Cost calculation (Cross Entropy)\n",
        "        m = Y.shape[1]\n",
        "    \n",
        "        logprobs = np.multiply(-np.log(Y_pred),Y) + np.multiply(-np.log(1 - Y_pred), 1 - Y)\n",
        "        cost = 1./m * np.nansum(logprobs)  \n",
        "\n",
        "        # Backward propagation\n",
        "        m = X.shape[1]\n",
        "        \n",
        "        dZ3 = Y_pred - Y\n",
        "        dW3 = 1./m * np.dot(dZ3, A2.T)\n",
        "        \n",
        "        dA2 = np.dot(W3.T, dZ3)\n",
        "        dZ2 = np.multiply(dA2, np.int64(A2 > 0))\n",
        "        dW2 = 1./m * np.dot(dZ2, A1.T)\n",
        "        \n",
        "        dA1 = np.dot(W2.T, dZ2)\n",
        "        dZ1 = np.multiply(dA1, np.int64(A1 > 0))\n",
        "        dW1 = 1./m * np.dot(dZ1, X.T)\n",
        "        \n",
        "        grads = {\"dZ3\": dZ3, \"dW3\": dW3, \n",
        "                 \"dA2\": dA2, \"dZ2\": dZ2, \"dW2\": dW2, \n",
        "                 \"dA1\": dA1, \"dZ1\": dZ1, \"dW1\": dW1,}\n",
        "\n",
        "        # Weight Updation\n",
        "        for k in range(len(parameters)//2):\n",
        "            parameters[\"W\" + str(k+1)] = parameters[\"W\" + str(k+1)] - learning_rate * grads[\"dW\" + str(k+1)]\n",
        "            \n",
        "        print(\"Cost after iteration {}: {}\".format(i, cost))\n",
        "        train_costs.append(cost)\n",
        "    \n",
        "    # Plot the cost\n",
        "    plt.plot(train_costs, color = 'blue')\n",
        "    plt.ylabel('training_cost')\n",
        "    plt.xlabel('iterations')\n",
        "    plt.title(\"Learning rate =\" + str(learning_rate))\n",
        "    plt.show()\n",
        "\n",
        "    return parameters"
      ],
      "metadata": {
        "id": "VIbi4D_RWa64"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "parameters = model(train_data, train_label, learning_rate = 0.1, num_iterations = 200, activation = relu)"
      ],
      "metadata": {
        "id": "pLdC6ZvlXgFo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Predict Function\n",
        "def predict(X, Y, parameters, activation = relu):\n",
        "    W1 = parameters[\"W1\"]\n",
        "    W2 = parameters[\"W2\"]\n",
        "    W3 = parameters[\"W3\"]\n",
        "\n",
        "    Z1 = np.dot(W1, X)\n",
        "    A1 = activation(Z1)\n",
        "    Z2 = np.dot(W2, A1)\n",
        "    A2 = activation(Z2)\n",
        "    Z3 = np.dot(W3, A2)\n",
        "    Y_pred = softmax(Z3)\n",
        "\n",
        "    prediction = np.argmax(Y_pred, axis = 0)\n",
        "    return prediction\n",
        "\n",
        "# Calculate Training and Test Accuracy\n",
        "train_pred = predict(train_data, train_label, parameters)\n",
        "print (\"On the training set:\")\n",
        "print(\"Accuracy: \"  + str(np.mean((train_pred[:] == np.argmax(train_label, axis = 0)))))\n",
        "\n",
        "test_pred = predict(test_data, test_label, parameters)\n",
        "print (\"On the test set:\")\n",
        "print(\"Accuracy: \"  + str(np.mean((test_pred[:] == np.argmax(test_label, axis = 0)))))"
      ],
      "metadata": {
        "id": "IbtCQGjBYRea"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "\n",
        "cm = confusion_matrix(np.argmax(train_label, axis = 0), test_pred)\n",
        "cm_df = pd.DataFrame(cm)\n",
        "\n",
        "plt.figure(figsize=(5,4))\n",
        "sns.heatmap(cm_df, annot=True)\n",
        "plt.title('Confusion Matrix_tune')\n",
        "plt.ylabel('Actal Values')\n",
        "plt.xlabel('Predicted Values')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "HBdJSqVuZhfX"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}