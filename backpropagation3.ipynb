{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1ojTexrzUF01qW86LoWy3MRJeIPTWM3b7",
      "authorship_tag": "ABX9TyM6ELIEgxBL2xK8DXuP6Dh7",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dongjaeseo/colab/blob/main/backpropagation3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VWI08qkKV9IP",
        "outputId": "b72488eb-d167-4b3d-a365-d589c0f17134"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from matplotlib import pyplot as plt"
      ],
      "metadata": {
        "id": "WaLOA4gUWRFC"
      },
      "execution_count": 64,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 데이터셋 불러오기\n",
        "# 데이터 경로에 맞게 경로 수정해주기\n",
        "\n",
        "train_data = np.loadtxt('/content/drive/MyDrive/data/backprop/training.dat', unpack = True)\n",
        "test_data = np.loadtxt('/content/drive/MyDrive/data/backprop/testing.dat', unpack = True)"
      ],
      "metadata": {
        "id": "h-QxXca3WSQD"
      },
      "execution_count": 65,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 트레인, 테스트 데이터 75개\n",
        "# 25개씩 나눠져서 총 3개의 클래스가 있다\n",
        "# 다중분류 하는 모델을 만들것이기 때문에 원-핫 인코딩을 해주었다\n",
        "# >> 라벨값이 0 = [1, 0, 0] / 1 = [0, 1, 0] / 2 = [0, 0, 1]\n",
        "\n",
        "train_label = []\n",
        "test_label = []\n",
        "for i in range(3):\n",
        "    for j in range(25):\n",
        "        label = np.zeros(3)\n",
        "        label[i] = 1\n",
        "        train_label.append(label)\n",
        "        test_label.append(label)\n",
        "    \n",
        "train_label = np.array(train_label).T\n",
        "test_label = np.array(test_label).T"
      ],
      "metadata": {
        "id": "V8jFcl9JWVLB"
      },
      "execution_count": 66,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 데이터의 쉐이프를 찍어보자\n",
        "# 트레인은 4, 75 / 테스트는 3, 75\n",
        "\n",
        "print(train_data.shape) # (4, 75)\n",
        "print(test_data.shape) # (3, 75)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GhPe3uWb13UC",
        "outputId": "4d2bebc2-20df-4765-a4c9-fd47a7e11f10"
      },
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(4, 75)\n",
            "(4, 75)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 사용할 활성화 함수\n",
        "# 다중분류를 하기 위해 마지막 레이어에서 사용할 소프트맥스 함수\n",
        "# 일반적으로 사용 시 성능이 좋은 ReLu 활성화 함수를 사용하였다\n",
        "\n",
        "def relu(x):\n",
        "    return np.maximum(0,x)\n",
        "\n",
        "def softmax(x):\n",
        "    return np.exp(x)/np.sum(np.exp(x),axis=0)"
      ],
      "metadata": {
        "id": "KfEynpbhWaer"
      },
      "execution_count": 68,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 모델을 정의 해주었다\n",
        "# 파라미터로는 트레인_X, 트레인_Y, learning_rate, 에폭, 사용할 활성화 함수가 있다\n",
        "\n",
        "def model(X, Y, learning_rate = 0.1, num_iterations = 20, activation = relu):\n",
        "\n",
        "    # 모델의 구조를 먼저 정의해주자\n",
        "    # 인풋으로는 X.shape[0] 즉 4 / 히든레이어는 100, 100 / 아웃풋은 소프트맥스를 통해 3개의 클라스 중 분류하기 때문에 3\n",
        "    layer_dims = [X.shape[0], 100, 100, 3]  \n",
        "\n",
        "    # grads 라는 딕셔너리에는 미분값을 담을것이다\n",
        "    grads = {}\n",
        "    train_costs = []\n",
        "    test_costs = []\n",
        "\n",
        "    # parameters 라는 딕셔너리에는 가중치값을 담을것이다\n",
        "    # 추후 두개의 딕셔너리를 통해 가중치에서 미분값을 연산하여 가중치를 조정해준다\n",
        "    parameters = {}\n",
        "\n",
        "    # 가중치 초기화\n",
        "    # Glorot Initialization 이라고 하는 가중치 초기화 방법이다\n",
        "    # 통상적으로 쓰이는 초기화 방식이나 그냥 0 으로 초기화 하셔도 학습 자체에는 문제가 없습니다\n",
        "    for i in range(1, len(layer_dims)):\n",
        "        m = np.sqrt(6/(layer_dims[i]+layer_dims[i-1]))\n",
        "        parameters['W' + str(i)] = np.random.uniform(-m, m, size = (layer_dims[i], layer_dims[i-1]))\n",
        "    \n",
        "    # num_iteration 에폭 \n",
        "    for i in range(0, num_iterations):  \n",
        "        '''\n",
        "        '''    \n",
        "\n",
        "        # 순전파\n",
        "        # 먼저 W1, W2, W3 를 정의해줍니다\n",
        "        # 각각 인풋-히든1 / 히든1-히든2 / 히든2-아웃풋 을 연결해주는 가중치값입니다\n",
        "        W1 = parameters[\"W1\"]\n",
        "        W2 = parameters[\"W2\"]\n",
        "        W3 = parameters[\"W3\"]\n",
        "\n",
        "        # W1, 인풋을 연산 / 히든1이 활성화 함수를 거치기전\n",
        "        Z1 = np.dot(W1, X)\n",
        "        # A1은 Z1이 활성화 함수를 거친 후 히든1 에서 나오는 값 \n",
        "        A1 = activation(Z1)\n",
        "\n",
        "        # W2, A1을 연산 / 히든2가 활성화 함수를 거치기전\n",
        "        Z2 = np.dot(W2, A1)\n",
        "        # A2는 Z2가 활성화 함수를 거친 후 히든2 에서 나오는 값\n",
        "        A2 = activation(Z2)\n",
        "\n",
        "        # W3, A2을 연산 / 최종 아웃풋이 활성화 함수를 거치기전\n",
        "        Z3 = np.dot(W3, A2)\n",
        "        # 아웃풋에 소프트 맥스 함수를 거쳐 총 3개의 값이 나온다\n",
        "        # 소프트맥스 함수 특징:\n",
        "        # 3개의 값이라고 할때 3개를 합치면 1 >> [0.1, 0.1, 0.8] 과 같이 모든 값을 합치면 1이 나온다\n",
        "        # 추후에 예측과정에서 제일 높은 숫자를 가진 인덱스를 추출할것 - [0.1, 0.1, 0.8] 라면 0.8이 제일 높은 확률을 가지기에 예측결과는 [0, 0, 1] 즉 2라고 할수있다\n",
        "        Y_pred = softmax(Z3)\n",
        "\n",
        "        '''\n",
        "        '''\n",
        "        # 비용함수\n",
        "        # 다중 분류에서 쓰이는 비용 함수는 Cross-Entropy 라고 한다\n",
        "        # 간단하게 설명하면 소프트맥스 함수를 거친 최종 아웃풋값이 자신의 라벨값과 비슷해질수록 낮아진다!\n",
        "        # >>  실제 라벨이 [0, 0, 1] 이라면 / [0.1, 0.1, 0.8] 이 [0.2, 0.2, 0.6] 보다 낮은 로스값을 가진다\n",
        "        m = Y.shape[1]\n",
        "    \n",
        "        logprobs = np.multiply(-np.log(Y_pred),Y) + np.multiply(-np.log(1 - Y_pred), 1 - Y)\n",
        "        cost = 1./m * np.nansum(logprobs)  \n",
        "\n",
        "        '''\n",
        "        '''\n",
        "\n",
        "        # 역전파\n",
        "        m = X.shape[1]\n",
        "        \n",
        "        # dZ3 는 최종 아웃풋과 실제값의 차이\n",
        "        # W3 를 조정해주려면 dW3 값을 계산해야한다\n",
        "        dZ3 = Y_pred - Y\n",
        "        dW3 = 1./m * np.dot(dZ3, A2.T)\n",
        "        \n",
        "\n",
        "        dA2 = np.dot(W3.T, dZ3)\n",
        "        dZ2 = np.multiply(dA2, np.int64(A2 > 0))\n",
        "        dW2 = 1./m * np.dot(dZ2, A1.T)\n",
        "        \n",
        "        dA1 = np.dot(W2.T, dZ2)\n",
        "        dZ1 = np.multiply(dA1, np.int64(A1 > 0))\n",
        "        dW1 = 1./m * np.dot(dZ1, X.T)\n",
        "        \n",
        "        grads = {\"dZ3\": dZ3, \"dW3\": dW3, \n",
        "                 \"dA2\": dA2, \"dZ2\": dZ2, \"dW2\": dW2, \n",
        "                 \"dA1\": dA1, \"dZ1\": dZ1, \"dW1\": dW1,}\n",
        "\n",
        "        '''\n",
        "        '''\n",
        "        # 가중치 업데이트\n",
        "        # grads 에는 미분값이\n",
        "        # parameters 에는 가중치값이 들어있고 이를 토대로\n",
        "        # W = W - lr*dW  식을 이용해서 가중치를 업데이트 해준다\n",
        "        for k in range(len(parameters)//2):\n",
        "            parameters[\"W\" + str(k+1)] = parameters[\"W\" + str(k+1)] - learning_rate * grads[\"dW\" + str(k+1)]\n",
        "        \n",
        "        # 최종적으로 구해진 오차값을 저장\n",
        "        print(\"Cost after iteration {}: {}\".format(i, cost))\n",
        "        train_costs.append(cost)\n",
        "    \n",
        "    # 저장된 오차값을 플롯하는 코드\n",
        "    # X축은 에폭 / Y축은 오차값이 출력\n",
        "    plt.plot(train_costs, color = 'blue')\n",
        "    plt.ylabel('training_cost')\n",
        "    plt.xlabel('iterations')\n",
        "    plt.title(\"Learning rate =\" + str(learning_rate))\n",
        "    plt.show()\n",
        "\n",
        "    # 최종적으로 구해진 가중치값을 리턴해준다\n",
        "    return parameters"
      ],
      "metadata": {
        "id": "VIbi4D_RWa64"
      },
      "execution_count": 69,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "parameters = model(train_data, train_label, learning_rate = 0.05, num_iterations = 150, activation = relu)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "pLdC6ZvlXgFo",
        "outputId": "baf60eb0-8073-4d02-cdbe-9db53c5162c0"
      },
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cost after iteration 0: 2.04550159823411\n",
            "Cost after iteration 1: 1.9363887619709417\n",
            "Cost after iteration 2: 1.854624168943871\n",
            "Cost after iteration 3: 1.8023054799976177\n",
            "Cost after iteration 4: 1.7571164110911999\n",
            "Cost after iteration 5: 1.7148610279037524\n",
            "Cost after iteration 6: 1.6783775998302939\n",
            "Cost after iteration 7: 1.6474916813399343\n",
            "Cost after iteration 8: 1.6194210070864516\n",
            "Cost after iteration 9: 1.5940189646314025\n",
            "Cost after iteration 10: 1.569678337776507\n",
            "Cost after iteration 11: 1.5465918411522972\n",
            "Cost after iteration 12: 1.524965596823077\n",
            "Cost after iteration 13: 1.5045700428962658\n",
            "Cost after iteration 14: 1.4847640411038812\n",
            "Cost after iteration 15: 1.4657486254262815\n",
            "Cost after iteration 16: 1.447024382349617\n",
            "Cost after iteration 17: 1.4295948903416116\n",
            "Cost after iteration 18: 1.4129461542201263\n",
            "Cost after iteration 19: 1.397157397343881\n",
            "Cost after iteration 20: 1.3822896489402994\n",
            "Cost after iteration 21: 1.3679215287974542\n",
            "Cost after iteration 22: 1.3542267636921184\n",
            "Cost after iteration 23: 1.3408830925787536\n",
            "Cost after iteration 24: 1.3280345804630749\n",
            "Cost after iteration 25: 1.3157495570489812\n",
            "Cost after iteration 26: 1.303930438063804\n",
            "Cost after iteration 27: 1.2928396241065598\n",
            "Cost after iteration 28: 1.2823455180884094\n",
            "Cost after iteration 29: 1.2722500447484186\n",
            "Cost after iteration 30: 1.26248896723474\n",
            "Cost after iteration 31: 1.2532309873178062\n",
            "Cost after iteration 32: 1.2442967339280424\n",
            "Cost after iteration 33: 1.2356426788218071\n",
            "Cost after iteration 34: 1.227152670389176\n",
            "Cost after iteration 35: 1.2190974267773462\n",
            "Cost after iteration 36: 1.2109838834981312\n",
            "Cost after iteration 37: 1.2030111243606187\n",
            "Cost after iteration 38: 1.1947172176175282\n",
            "Cost after iteration 39: 1.1864049096437652\n",
            "Cost after iteration 40: 1.1782154491477832\n",
            "Cost after iteration 41: 1.17021976915677\n",
            "Cost after iteration 42: 1.1624181764724446\n",
            "Cost after iteration 43: 1.154747812057571\n",
            "Cost after iteration 44: 1.1473305982076667\n",
            "Cost after iteration 45: 1.1398577236676721\n",
            "Cost after iteration 46: 1.1324136430832024\n",
            "Cost after iteration 47: 1.1245020075023946\n",
            "Cost after iteration 48: 1.1174765223843774\n",
            "Cost after iteration 49: 1.1108511395619487\n",
            "Cost after iteration 50: 1.1043005223819256\n",
            "Cost after iteration 51: 1.0977873394190336\n",
            "Cost after iteration 52: 1.0908885290154873\n",
            "Cost after iteration 53: 1.0831543297234165\n",
            "Cost after iteration 54: 1.0748997835517697\n",
            "Cost after iteration 55: 1.0686196569401973\n",
            "Cost after iteration 56: 1.0630511261247397\n",
            "Cost after iteration 57: 1.057706529347319\n",
            "Cost after iteration 58: 1.052443584915724\n",
            "Cost after iteration 59: 1.04734019823453\n",
            "Cost after iteration 60: 1.042333824159481\n",
            "Cost after iteration 61: 1.0373726635130696\n",
            "Cost after iteration 62: 1.0327159819818978\n",
            "Cost after iteration 63: 1.0281034423860609\n",
            "Cost after iteration 64: 1.0236687550563963\n",
            "Cost after iteration 65: 1.0194864190325532\n",
            "Cost after iteration 66: 1.015366914690237\n",
            "Cost after iteration 67: 1.0112903485274682\n",
            "Cost after iteration 68: 1.0073150682083953\n",
            "Cost after iteration 69: 1.0034107528076217\n",
            "Cost after iteration 70: 0.9996147637804268\n",
            "Cost after iteration 71: 0.9957988599428803\n",
            "Cost after iteration 72: 0.9921912245423272\n",
            "Cost after iteration 73: 0.9885010846052568\n",
            "Cost after iteration 74: 0.984921673439019\n",
            "Cost after iteration 75: 0.9812845445203179\n",
            "Cost after iteration 76: 0.9777373352029338\n",
            "Cost after iteration 77: 0.9737666068584573\n",
            "Cost after iteration 78: 0.96928290174343\n",
            "Cost after iteration 79: 0.9645278152280066\n",
            "Cost after iteration 80: 0.9606741958135441\n",
            "Cost after iteration 81: 0.9570340524178431\n",
            "Cost after iteration 82: 0.9535979191504835\n",
            "Cost after iteration 83: 0.9500292992762344\n",
            "Cost after iteration 84: 0.9468178150405137\n",
            "Cost after iteration 85: 0.9433420792548715\n",
            "Cost after iteration 86: 0.9402185811528893\n",
            "Cost after iteration 87: 0.9368503911504422\n",
            "Cost after iteration 88: 0.9336925237360345\n",
            "Cost after iteration 89: 0.9305665481080155\n",
            "Cost after iteration 90: 0.9274897251292044\n",
            "Cost after iteration 91: 0.9243699285089039\n",
            "Cost after iteration 92: 0.9212040373969305\n",
            "Cost after iteration 93: 0.918145873769589\n",
            "Cost after iteration 94: 0.9148551137941627\n",
            "Cost after iteration 95: 0.9116659449817432\n",
            "Cost after iteration 96: 0.9082245581997048\n",
            "Cost after iteration 97: 0.9043344383047041\n",
            "Cost after iteration 98: 0.9008212598860176\n",
            "Cost after iteration 99: 0.8977107220215347\n",
            "Cost after iteration 100: 0.8946549507385307\n",
            "Cost after iteration 101: 0.8916001166461055\n",
            "Cost after iteration 102: 0.8887686954059556\n",
            "Cost after iteration 103: 0.8857997542737862\n",
            "Cost after iteration 104: 0.8829876463416585\n",
            "Cost after iteration 105: 0.8801525983901722\n",
            "Cost after iteration 106: 0.8774223891726018\n",
            "Cost after iteration 107: 0.8746285340960863\n",
            "Cost after iteration 108: 0.871920061341499\n",
            "Cost after iteration 109: 0.8691591249138207\n",
            "Cost after iteration 110: 0.8664808424564439\n",
            "Cost after iteration 111: 0.8636981718853821\n",
            "Cost after iteration 112: 0.8608205428985526\n",
            "Cost after iteration 113: 0.8579413714776638\n",
            "Cost after iteration 114: 0.8552464393434384\n",
            "Cost after iteration 115: 0.8524976433054322\n",
            "Cost after iteration 116: 0.8498699517747825\n",
            "Cost after iteration 117: 0.8471447782620631\n",
            "Cost after iteration 118: 0.8445674148887874\n",
            "Cost after iteration 119: 0.8418056338246293\n",
            "Cost after iteration 120: 0.8392768618261841\n",
            "Cost after iteration 121: 0.836568243100179\n",
            "Cost after iteration 122: 0.8340629480148143\n",
            "Cost after iteration 123: 0.831439237340664\n",
            "Cost after iteration 124: 0.8290022128444565\n",
            "Cost after iteration 125: 0.8263279312370274\n",
            "Cost after iteration 126: 0.8237493166557727\n",
            "Cost after iteration 127: 0.8209958984889213\n",
            "Cost after iteration 128: 0.8183333450188559\n",
            "Cost after iteration 129: 0.8156421478136914\n",
            "Cost after iteration 130: 0.8128475638003102\n",
            "Cost after iteration 131: 0.8095715099744336\n",
            "Cost after iteration 132: 0.8065155753028749\n",
            "Cost after iteration 133: 0.8036958408312114\n",
            "Cost after iteration 134: 0.8010092412740933\n",
            "Cost after iteration 135: 0.7984932086711873\n",
            "Cost after iteration 136: 0.795900174441199\n",
            "Cost after iteration 137: 0.7934925255890549\n",
            "Cost after iteration 138: 0.7911914440183964\n",
            "Cost after iteration 139: 0.7889842278516932\n",
            "Cost after iteration 140: 0.7868185966278951\n",
            "Cost after iteration 141: 0.7845772152869456\n",
            "Cost after iteration 142: 0.7824801855776694\n",
            "Cost after iteration 143: 0.7803254788436834\n",
            "Cost after iteration 144: 0.7783100935685675\n",
            "Cost after iteration 145: 0.7761807409206328\n",
            "Cost after iteration 146: 0.7742127052610467\n",
            "Cost after iteration 147: 0.7721827103104797\n",
            "Cost after iteration 148: 0.7702561183615535\n",
            "Cost after iteration 149: 0.768303647217281\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 예측 함수\n",
        "# 저장된 가중치값을 가지고 있기 때문에\n",
        "# 테스트 데이터를 이 가중치값으로 순전파를 통과시켜주면 모델의 예측값이 나온다\n",
        "def predict(X, Y, parameters, activation = relu):\n",
        "    W1 = parameters[\"W1\"]\n",
        "    W2 = parameters[\"W2\"]\n",
        "    W3 = parameters[\"W3\"]\n",
        "\n",
        "    # 위에 훈련시 사용한 순전파 모델\n",
        "    Z1 = np.dot(W1, X)\n",
        "    A1 = activation(Z1)\n",
        "    Z2 = np.dot(W2, A1)\n",
        "    A2 = activation(Z2)\n",
        "    Z3 = np.dot(W3, A2)\n",
        "    Y_pred = softmax(Z3)\n",
        "\n",
        "    # 이렇게 모델의 예측값인 Y_pred 값은 그대로 사용할수는 없다\n",
        "    # 총 3개의 데이터로 이뤄져 있기 때문\n",
        "    # [0.1, 0.2, 0.7] 이런식으로 각 클래스에 대한 확률이 나와있다\n",
        "    # 그래서 여기서 제일 큰 확률을 가진 클래스를 찾아주는 코드다\n",
        "    prediction = np.argmax(Y_pred, axis = 0)\n",
        "    return prediction\n",
        "\n",
        "# 이렇게 얻은 모델 가중치로 훈련데이터, 테스트 데이터를 각각 예측해본 결과\n",
        "'''\n",
        "On the training set:\n",
        "Accuracy: 0.9466666666666667\n",
        "On the test set:\n",
        "Accuracy: 0.9733333333333334\n",
        "'''\n",
        "# 훈련데이터에는 96 퍼센트의 정확도를 보였고\n",
        "# 테스트데이터에는 89.33 퍼센트의 정확도를 보였다\n",
        "\n",
        "# 아래는 정확도를 보게 해주는 코드\n",
        "train_pred = predict(train_data, train_label, parameters)\n",
        "print (\"On the training set:\")\n",
        "print(\"Accuracy: \"  + str(np.mean((train_pred[:] == np.argmax(train_label, axis = 0)))))\n",
        "\n",
        "test_pred = predict(test_data, test_label, parameters)\n",
        "print (\"On the test set:\")\n",
        "print(\"Accuracy: \"  + str(np.mean((test_pred[:] == np.argmax(test_label, axis = 0)))))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IbtCQGjBYRea",
        "outputId": "a614576d-0b88-448c-8c48-85dbf3184d32"
      },
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "On the training set:\n",
            "Accuracy: 0.96\n",
            "On the test set:\n",
            "Accuracy: 0.96\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install nbconvert\n",
        "!sudo apt-get install texlive-xetex texlive-fonts-recommended texlive-plain-generic\n",
        "!jupyter nbconvert --to html \"/content/drive/MyDrive/Colab Notebooks/backpropagation3.ipynb\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qmHewH2o-feC",
        "outputId": "445dbd1b-f0f8-4207-f76f-110256811b80"
      },
      "execution_count": 76,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: nbconvert in /usr/local/lib/python3.7/dist-packages (5.6.1)\n",
            "Requirement already satisfied: mistune<2,>=0.8.1 in /usr/local/lib/python3.7/dist-packages (from nbconvert) (0.8.4)\n",
            "Requirement already satisfied: defusedxml in /usr/local/lib/python3.7/dist-packages (from nbconvert) (0.7.1)\n",
            "Requirement already satisfied: entrypoints>=0.2.2 in /usr/local/lib/python3.7/dist-packages (from nbconvert) (0.4)\n",
            "Requirement already satisfied: bleach in /usr/local/lib/python3.7/dist-packages (from nbconvert) (5.0.1)\n",
            "Requirement already satisfied: testpath in /usr/local/lib/python3.7/dist-packages (from nbconvert) (0.6.0)\n",
            "Requirement already satisfied: jupyter-core in /usr/local/lib/python3.7/dist-packages (from nbconvert) (4.11.2)\n",
            "Requirement already satisfied: nbformat>=4.4 in /usr/local/lib/python3.7/dist-packages (from nbconvert) (5.7.0)\n",
            "Requirement already satisfied: pygments in /usr/local/lib/python3.7/dist-packages (from nbconvert) (2.6.1)\n",
            "Requirement already satisfied: pandocfilters>=1.4.1 in /usr/local/lib/python3.7/dist-packages (from nbconvert) (1.5.0)\n",
            "Requirement already satisfied: traitlets>=4.2 in /usr/local/lib/python3.7/dist-packages (from nbconvert) (5.1.1)\n",
            "Requirement already satisfied: jinja2>=2.4 in /usr/local/lib/python3.7/dist-packages (from nbconvert) (2.11.3)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from jinja2>=2.4->nbconvert) (2.0.1)\n",
            "Requirement already satisfied: jsonschema>=2.6 in /usr/local/lib/python3.7/dist-packages (from nbformat>=4.4->nbconvert) (4.3.3)\n",
            "Requirement already satisfied: fastjsonschema in /usr/local/lib/python3.7/dist-packages (from nbformat>=4.4->nbconvert) (2.16.2)\n",
            "Requirement already satisfied: importlib-metadata>=3.6 in /usr/local/lib/python3.7/dist-packages (from nbformat>=4.4->nbconvert) (4.13.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=3.6->nbformat>=4.4->nbconvert) (4.1.1)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=3.6->nbformat>=4.4->nbconvert) (3.10.0)\n",
            "Requirement already satisfied: pyrsistent!=0.17.0,!=0.17.1,!=0.17.2,>=0.14.0 in /usr/local/lib/python3.7/dist-packages (from jsonschema>=2.6->nbformat>=4.4->nbconvert) (0.19.2)\n",
            "Requirement already satisfied: importlib-resources>=1.4.0 in /usr/local/lib/python3.7/dist-packages (from jsonschema>=2.6->nbformat>=4.4->nbconvert) (5.10.0)\n",
            "Requirement already satisfied: attrs>=17.4.0 in /usr/local/lib/python3.7/dist-packages (from jsonschema>=2.6->nbformat>=4.4->nbconvert) (22.1.0)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.7/dist-packages (from bleach->nbconvert) (1.15.0)\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.7/dist-packages (from bleach->nbconvert) (0.5.1)\n",
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "texlive-fonts-recommended is already the newest version (2017.20180305-1).\n",
            "texlive-plain-generic is already the newest version (2017.20180305-2).\n",
            "texlive-xetex is already the newest version (2017.20180305-1).\n",
            "The following package was automatically installed and is no longer required:\n",
            "  libnvidia-common-460\n",
            "Use 'sudo apt autoremove' to remove it.\n",
            "0 upgraded, 0 newly installed, 0 to remove and 5 not upgraded.\n",
            "[NbConvertApp] Converting notebook /content/drive/MyDrive/Colab Notebooks/backpropagation3.ipynb to html\n",
            "[NbConvertApp] Writing 327917 bytes to /content/drive/MyDrive/Colab Notebooks/backpropagation3.html\n"
          ]
        }
      ]
    }
  ]
}