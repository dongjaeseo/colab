{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "authorship_tag": "ABX9TyOHrsj+CxvIxk1oOSB8YiKX",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dongjaeseo/colab/blob/main/backpropagation3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VWI08qkKV9IP"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from matplotlib import pyplot as plt"
      ],
      "metadata": {
        "id": "WaLOA4gUWRFC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 데이터셋 불러오기\n",
        "# 데이터 경로에 맞게 경로 수정해주기\n",
        "\n",
        "train_data = np.loadtxt('/content/drive/MyDrive/data/backprop/training.dat', unpack = True)\n",
        "test_data = np.loadtxt('/content/drive/MyDrive/data/backprop/testing.dat', unpack = True)"
      ],
      "metadata": {
        "id": "h-QxXca3WSQD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 트레인, 테스트 데이터 75개\n",
        "# 25개씩 나눠져서 총 3개의 클래스가 있다\n",
        "# 다중분류 하는 모델을 만들것이기 때문에 원-핫 인코딩을 해주었다\n",
        "# >> 라벨값이 0 = [1, 0, 0] / 1 = [0, 1, 0] / 2 = [0, 0, 1]\n",
        "\n",
        "train_label = []\n",
        "test_label = []\n",
        "for i in range(3):\n",
        "    for j in range(25):\n",
        "        label = np.zeros(3)\n",
        "        label[i] = 1\n",
        "        train_label.append(label)\n",
        "        test_label.append(label)\n",
        "    \n",
        "train_label = np.array(train_label).T\n",
        "test_label = np.array(test_label).T"
      ],
      "metadata": {
        "id": "V8jFcl9JWVLB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 데이터의 쉐이프를 찍어보자\n",
        "# 트레인은 4, 75 / 테스트는 3, 75\n",
        "\n",
        "print(train_data.shape) # (4, 75)\n",
        "print(test_data.shape) # (3, 75)"
      ],
      "metadata": {
        "id": "GhPe3uWb13UC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 사용할 활성화 함수\n",
        "# 다중분류를 하기 위해 마지막 레이어에서 사용할 소프트맥스 함수\n",
        "# 일반적으로 사용 시 성능이 좋은 ReLu 활성화 함수를 사용하였다\n",
        "\n",
        "def relu(x):\n",
        "    return np.maximum(0,x)\n",
        "\n",
        "def softmax(x):\n",
        "    return np.exp(x)/np.sum(np.exp(x),axis=0)"
      ],
      "metadata": {
        "id": "KfEynpbhWaer"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 모델을 정의 해주었다\n",
        "# 파라미터로는 트레인_X, 트레인_Y, learning_rate, 에폭, 사용할 활성화 함수가 있다\n",
        "\n",
        "def model(X, Y, learning_rate = 0.1, num_iterations = 20, activation = relu):\n",
        "\n",
        "    # 모델의 구조를 먼저 정의해주자\n",
        "    # 인풋으로는 X.shape[0] 즉 4 / 히든레이어는 4, 4 / 아웃풋은 소프트맥스를 통해 3개의 클라스 중 분류하기 때문에 3\n",
        "    layer_dims = [X.shape[0], 4, 4, 3]  \n",
        "\n",
        "    # grads 라는 딕셔너리에는 미분값을 담을것이다\n",
        "    grads = {}\n",
        "    train_costs = []\n",
        "    test_costs = []\n",
        "\n",
        "    # parameters 라는 딕셔너리에는 가중치값을 담을것이다\n",
        "    # 추후 두개의 딕셔너리를 통해 가중치에서 미분값을 연산하여 가중치를 조정해준다\n",
        "    parameters = {}\n",
        "\n",
        "    # 가중치 초기화\n",
        "    # Glorot Initialization 이라고 하는 가중치 초기화 방법이다\n",
        "    # 통상적으로 쓰이는 초기화 방식이나 그냥 0 으로 초기화 하셔도 학습 자체에는 문제가 없습니다\n",
        "    for i in range(1, len(layer_dims)):\n",
        "        m = np.sqrt(6/(layer_dims[i]+layer_dims[i-1]))\n",
        "        parameters['W' + str(i)] = np.random.uniform(-m, m, size = (layer_dims[i], layer_dims[i-1]))\n",
        "    \n",
        "    # num_iteration 에폭 \n",
        "    for i in range(0, num_iterations):  \n",
        "        '''\n",
        "        '''    \n",
        "\n",
        "        # 순전파\n",
        "        # 먼저 W1, W2, W3 를 정의해줍니다\n",
        "        # 각각 인풋-히든1 / 히든1-히든2 / 히든2-아웃풋 을 연결해주는 가중치값입니다\n",
        "        W1 = parameters[\"W1\"]\n",
        "        W2 = parameters[\"W2\"]\n",
        "        W3 = parameters[\"W3\"]\n",
        "\n",
        "        # W1, 인풋을 연산 / 히든1이 활성화 함수를 거치기전\n",
        "        Z1 = np.dot(W1, X)\n",
        "        # A1은 Z1이 활성화 함수를 거친 후 히든1 에서 나오는 값 \n",
        "        A1 = activation(Z1)\n",
        "\n",
        "        # W2, A1을 연산 / 히든2가 활성화 함수를 거치기전\n",
        "        Z2 = np.dot(W2, A1)\n",
        "        # A2는 Z2가 활성화 함수를 거친 후 히든2 에서 나오는 값\n",
        "        A2 = activation(Z2)\n",
        "\n",
        "        # W3, A2을 연산 / 최종 아웃풋이 활성화 함수를 거치기전\n",
        "        Z3 = np.dot(W3, A2)\n",
        "        # 아웃풋에 소프트 맥스 함수를 거쳐 총 3개의 값이 나온다\n",
        "        # 소프트맥스 함수 특징:\n",
        "        # 3개의 값이라고 할때 3개를 합치면 1 >> [0.1, 0.1, 0.8] 과 같이 모든 값을 합치면 1이 나온다\n",
        "        # 추후에 예측과정에서 제일 높은 숫자를 가진 인덱스를 추출할것 - [0.1, 0.1, 0.8] 라면 0.8이 제일 높은 확률을 가지기에 예측결과는 [0, 0, 1] 즉 2라고 할수있다\n",
        "        Y_pred = softmax(Z3)\n",
        "\n",
        "        '''\n",
        "        '''\n",
        "        # 비용함수\n",
        "        # 다중 분류에서 쓰이는 비용 함수는 Cross-Entropy 라고 한다\n",
        "        # 간단하게 설명하면 소프트맥스 함수를 거친 최종 아웃풋값이 자신의 라벨값과 비슷해질수록 낮아진다!\n",
        "        # >>  실제 라벨이 [0, 0, 1] 이라면 / [0.1, 0.1, 0.8] 이 [0.2, 0.2, 0.6] 보다 낮은 로스값을 가진다\n",
        "        m = Y.shape[1]\n",
        "    \n",
        "        logprobs = np.multiply(-np.log(Y_pred),Y) + np.multiply(-np.log(1 - Y_pred), 1 - Y)\n",
        "        cost = 1./m * np.nansum(logprobs)  \n",
        "\n",
        "        '''\n",
        "        '''\n",
        "\n",
        "        # 역전파\n",
        "        m = X.shape[1]\n",
        "        \n",
        "        # dZ3 는 최종 아웃풋과 실제값의 차이\n",
        "        # W3 를 조정해주려면 dW3 값을 계산해야한다\n",
        "        dZ3 = Y_pred - Y\n",
        "        dW3 = 1./m * np.dot(dZ3, A2.T)\n",
        "        \n",
        "\n",
        "        dA2 = np.dot(W3.T, dZ3)\n",
        "        dZ2 = np.multiply(dA2, np.int64(A2 > 0))\n",
        "        dW2 = 1./m * np.dot(dZ2, A1.T)\n",
        "        \n",
        "        dA1 = np.dot(W2.T, dZ2)\n",
        "        dZ1 = np.multiply(dA1, np.int64(A1 > 0))\n",
        "        dW1 = 1./m * np.dot(dZ1, X.T)\n",
        "        \n",
        "        grads = {\"dZ3\": dZ3, \"dW3\": dW3, \n",
        "                 \"dA2\": dA2, \"dZ2\": dZ2, \"dW2\": dW2, \n",
        "                 \"dA1\": dA1, \"dZ1\": dZ1, \"dW1\": dW1,}\n",
        "\n",
        "        '''\n",
        "        '''\n",
        "        # 가중치 업데이트\n",
        "        # grads 에는 미분값이\n",
        "        # parameters 에는 가중치값이 들어있고 이를 토대로\n",
        "        # W = W - lr*dW  식을 이용해서 가중치를 업데이트 해준다\n",
        "        for k in range(len(parameters)//2):\n",
        "            parameters[\"W\" + str(k+1)] = parameters[\"W\" + str(k+1)] - learning_rate * grads[\"dW\" + str(k+1)]\n",
        "        \n",
        "        # 최종적으로 구해진 오차값을 저장\n",
        "        print(\"Cost after iteration {}: {}\".format(i, cost))\n",
        "        train_costs.append(cost)\n",
        "    \n",
        "    # 저장된 오차값을 플롯하는 코드\n",
        "    # X축은 에폭 / Y축은 오차값이 출력\n",
        "    plt.plot(train_costs, color = 'blue')\n",
        "    plt.ylabel('training_cost')\n",
        "    plt.xlabel('iterations')\n",
        "    plt.title(\"Learning rate =\" + str(learning_rate))\n",
        "    plt.show()\n",
        "\n",
        "    # 최종적으로 구해진 가중치값을 리턴해준다\n",
        "    return parameters"
      ],
      "metadata": {
        "id": "VIbi4D_RWa64"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "parameters = model(train_data, train_label, learning_rate = 0.05, num_iterations = 150, activation = relu)"
      ],
      "metadata": {
        "id": "pLdC6ZvlXgFo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 예측 함수\n",
        "# 저장된 가중치값을 가지고 있기 때문에\n",
        "# 테스트 데이터를 이 가중치값으로 순전파를 통과시켜주면 모델의 예측값이 나온다\n",
        "def predict(X, Y, parameters, activation = relu):\n",
        "    W1 = parameters[\"W1\"]\n",
        "    W2 = parameters[\"W2\"]\n",
        "    W3 = parameters[\"W3\"]\n",
        "\n",
        "    # 위에 훈련시 사용한 순전파 모델\n",
        "    Z1 = np.dot(W1, X)\n",
        "    A1 = activation(Z1)\n",
        "    Z2 = np.dot(W2, A1)\n",
        "    A2 = activation(Z2)\n",
        "    Z3 = np.dot(W3, A2)\n",
        "    Y_pred = softmax(Z3)\n",
        "\n",
        "    # 이렇게 모델의 예측값인 Y_pred 값은 그대로 사용할수는 없다\n",
        "    # 총 3개의 데이터로 이뤄져 있기 때문\n",
        "    # [0.1, 0.2, 0.7] 이런식으로 각 클래스에 대한 확률이 나와있다\n",
        "    # 그래서 여기서 제일 큰 확률을 가진 클래스를 찾아주는 코드다\n",
        "    prediction = np.argmax(Y_pred, axis = 0)\n",
        "    return prediction\n",
        "\n",
        "# 이렇게 얻은 모델 가중치로 훈련데이터, 테스트 데이터를 각각 예측해본 결과\n",
        "'''\n",
        "On the training set:\n",
        "Accuracy: 0.96\n",
        "On the test set:\n",
        "Accuracy: 0.8933333333333333\n",
        "'''\n",
        "# 훈련데이터에는 96 퍼센트의 정확도를 보였고\n",
        "# 테스트데이터에는 89.33 퍼센트의 정확도를 보였다\n",
        "\n",
        "# 아래는 정확도를 보게 해주는 코드\n",
        "train_pred = predict(train_data, train_label, parameters)\n",
        "print (\"On the training set:\")\n",
        "print(\"Accuracy: \"  + str(np.mean((train_pred[:] == np.argmax(train_label, axis = 0)))))\n",
        "\n",
        "test_pred = predict(test_data, test_label, parameters)\n",
        "print (\"On the test set:\")\n",
        "print(\"Accuracy: \"  + str(np.mean((test_pred[:] == np.argmax(test_label, axis = 0)))))"
      ],
      "metadata": {
        "id": "IbtCQGjBYRea"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install nbconvert\n",
        "!sudo apt-get install texlive-xetex texlive-fonts-recommended texlive-plain-generic\n",
        "!jupyter nbconvert --to html \"/content/drive/MyDrive/Colab Notebooks/backpropagation3.ipynb.ipynb\""
      ],
      "metadata": {
        "id": "qmHewH2o-feC"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}