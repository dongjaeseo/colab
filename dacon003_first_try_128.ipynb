{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "dacon003_first_try_128.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "mount_file_id": "1dPtMl9zDkeH_5tU8kNtZgqJc4KkS6yBd",
      "authorship_tag": "ABX9TyMP1gf+q16J/oPlO2vbih9y"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "LGWQ94mP13Bp"
      },
      "source": [
        "import numpy as np\r\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\r\n",
        "from tensorflow.keras.models import Model, Sequential\r\n",
        "from tensorflow.keras.layers import Dense, Conv2D, MaxPooling2D, Dropout, BatchNormalization, Flatten\r\n",
        "from sklearn.model_selection import train_test_split\r\n",
        "from tensorflow.keras.callbacks import ReduceLROnPlateau, EarlyStopping, ModelCheckpoint\r\n",
        "import string\r\n",
        "import pandas as pd\r\n",
        "import cv2\r\n",
        "\r\n",
        "def mymodel():\r\n",
        "    model = Sequential()\r\n",
        "\r\n",
        "    model.add(Conv2D(16,(3,3),activation='relu',input_shape=(128,128,1),padding='same'))\r\n",
        "    model.add(BatchNormalization())\r\n",
        "    model.add(Dropout(0.3))\r\n",
        "    \r\n",
        "    model.add(Conv2D(32,(3,3),activation='relu',padding='same'))\r\n",
        "    model.add(BatchNormalization())\r\n",
        "    model.add(Conv2D(32,(5,5),activation='relu',padding='same')) \r\n",
        "    model.add(BatchNormalization())\r\n",
        "    model.add(Conv2D(32,(5,5),activation='relu',padding='same'))\r\n",
        "    model.add(BatchNormalization())\r\n",
        "    model.add(Conv2D(32,(5,5),activation='relu',padding='same'))\r\n",
        "    model.add(BatchNormalization())\r\n",
        "    model.add(MaxPooling2D((3,3)))\r\n",
        "    model.add(Dropout(0.3))\r\n",
        "    \r\n",
        "    model.add(Conv2D(64,(3,3),activation='relu',padding='same'))\r\n",
        "    model.add(BatchNormalization())\r\n",
        "    model.add(Conv2D(64,(5,5),activation='relu',padding='same')) \r\n",
        "    model.add(BatchNormalization())\r\n",
        "    model.add(MaxPooling2D((3,3)))\r\n",
        "    model.add(Dropout(0.3))\r\n",
        "    \r\n",
        "    model.add(Flatten())\r\n",
        "\r\n",
        "    model.add(Dense(128,activation='relu'))\r\n",
        "    model.add(BatchNormalization())\r\n",
        "    model.add(Dropout(0.3))\r\n",
        "    model.add(Dense(64,activation='relu'))\r\n",
        "    model.add(BatchNormalization())\r\n",
        "    model.add(Dropout(0.3))\r\n",
        "\r\n",
        "    model.add(Dense(1,activation='sigmoid'))\r\n",
        "    return model\r\n",
        "    \r\n",
        "#1. 데이터\r\n",
        "x_train = np.load('/content/drive/MyDrive/dacon/npy/x_train_128.npy')\r\n",
        "x_test = np.load('/content/drive/MyDrive/dacon/npy/x_test_128.npy')\r\n",
        "answer = pd.read_csv('/content/drive/MyDrive/dacon/dirty_mnist_answer.csv')\r\n",
        "sub = pd.read_csv('/content/drive/MyDrive/dacon/sample_submission.csv', header = 0)\r\n",
        "\r\n",
        "# print(x_train.shape) (50000, 128, 128, 1)\r\n",
        "# print(np.max(x_train[0])) # 1.0\r\n",
        "\r\n",
        "alphabets = string.ascii_lowercase\r\n",
        "alphabets = list(alphabets)\r\n",
        "\r\n",
        "val = []\r\n",
        "acc = []\r\n",
        "for alphabet in alphabets:\r\n",
        "    y_train = answer.loc[:,alphabet].to_numpy()\r\n",
        "\r\n",
        "    #2. 모델\r\n",
        "    model = mymodel()\r\n",
        "\r\n",
        "    #3. 컴파일 훈련\r\n",
        "    es = EarlyStopping(patience = 30)\r\n",
        "    lr = ReduceLROnPlateau(patience = 10, factor = 0.25, verbose = 1)\r\n",
        "    cp = ModelCheckpoint(f'/content/drive/MyDrive/dacon/mcp/{alphabet}.hdf5')\r\n",
        "    model.compile(loss = 'binary_crossentropy', optimizer= 'adam', metrics = ['acc'])\r\n",
        "    history = model.fit(x_train, y_train, epochs = 10000, batch_size = 128, validation_split = 0.2, callbacks = [es,cp,lr])\r\n",
        "    val.append(history.history['val_loss'][-29])\r\n",
        "    acc.append(history.history['val_acc'][-29])\r\n",
        "\r\n",
        "print('loss : ', val)\r\n",
        "print('acc : ', acc)\r\n",
        "    #4. 평가 예측\r\n",
        "#     y_pred = model.predict(x_test)\r\n",
        "#     y_pred = np.where(y_pred<0.5,0,1)\r\n",
        "#     sub.loc[:,alphabet] = y_pred\r\n",
        "\r\n",
        "# sub.to_csv('/content/drive/MyDrive/dacon/submission_003.csv', index = 0)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}