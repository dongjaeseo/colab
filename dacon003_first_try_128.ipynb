{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "dacon003_first_try_128.ipynb",
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "mount_file_id": "1dPtMl9zDkeH_5tU8kNtZgqJc4KkS6yBd",
      "authorship_tag": "ABX9TyMY4iNXNwlk04u7icKywKtB"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "LGWQ94mP13Bp"
      },
      "source": [
        "import numpy as np\r\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\r\n",
        "from tensorflow.keras.models import Model, Sequential\r\n",
        "from tensorflow.keras.layers import Dense, Conv2D, MaxPooling2D, Dropout, BatchNormalization, Flatten\r\n",
        "from sklearn.model_selection import train_test_split\r\n",
        "from tensorflow.keras.callbacks import ReduceLROnPlateau, EarlyStopping, ModelCheckpoint\r\n",
        "import string\r\n",
        "import pandas as pd\r\n",
        "import cv2\r\n",
        "\r\n",
        "def mymodel():\r\n",
        "    model = Sequential()\r\n",
        "\r\n",
        "    model.add(Conv2D(16,(3,3),activation='relu',input_shape=(128,128,1),padding='same'))\r\n",
        "    model.add(BatchNormalization())\r\n",
        "    model.add(Dropout(0.3))\r\n",
        "    \r\n",
        "    model.add(Conv2D(32,(3,3),activation='relu',padding='same'))\r\n",
        "    model.add(BatchNormalization())\r\n",
        "    model.add(Conv2D(32,(5,5),activation='relu',padding='same')) \r\n",
        "    model.add(BatchNormalization())\r\n",
        "    model.add(Conv2D(32,(5,5),activation='relu',padding='same'))\r\n",
        "    model.add(BatchNormalization())\r\n",
        "    model.add(Conv2D(32,(5,5),activation='relu',padding='same'))\r\n",
        "    model.add(BatchNormalization())\r\n",
        "    model.add(MaxPooling2D((3,3)))\r\n",
        "    model.add(Dropout(0.3))\r\n",
        "    \r\n",
        "    model.add(Conv2D(64,(3,3),activation='relu',padding='same'))\r\n",
        "    model.add(BatchNormalization())\r\n",
        "    model.add(Conv2D(64,(5,5),activation='relu',padding='same')) \r\n",
        "    model.add(BatchNormalization())\r\n",
        "    model.add(MaxPooling2D((3,3)))\r\n",
        "    model.add(Dropout(0.3))\r\n",
        "    \r\n",
        "    model.add(Flatten())\r\n",
        "\r\n",
        "    model.add(Dense(128,activation='relu'))\r\n",
        "    model.add(BatchNormalization())\r\n",
        "    model.add(Dropout(0.3))\r\n",
        "    model.add(Dense(64,activation='relu'))\r\n",
        "    model.add(BatchNormalization())\r\n",
        "    model.add(Dropout(0.3))\r\n",
        "\r\n",
        "    model.add(Dense(1,activation='sigmoid'))\r\n",
        "    return model\r\n",
        "    \r\n",
        "#1. 데이터\r\n",
        "x_train = np.load('/content/drive/MyDrive/dacon/npy/x_train_128.npy')\r\n",
        "x_test = np.load('/content/drive/MyDrive/dacon/npy/x_test_128.npy')\r\n",
        "answer = pd.read_csv('/content/drive/MyDrive/dacon/dirty_mnist_answer.csv')\r\n",
        "sub = pd.read_csv('/content/drive/MyDrive/dacon/sample_submission.csv', header = 0)\r\n",
        "\r\n",
        "# print(x_train.shape) (50000, 128, 128, 1)\r\n",
        "# print(np.max(x_train[0])) # 1.0\r\n",
        "\r\n",
        "alphabets = string.ascii_lowercase\r\n",
        "alphabets = list(alphabets)\r\n",
        "\r\n",
        "val = []\r\n",
        "acc = []\r\n",
        "for alphabet in alphabets:\r\n",
        "    y_train = answer.loc[:,alphabet].to_numpy()\r\n",
        "\r\n",
        "    #2. 모델\r\n",
        "    model = mymodel()\r\n",
        "\r\n",
        "    #3. 컴파일 훈련\r\n",
        "    es = EarlyStopping(patience = 30)\r\n",
        "    lr = ReduceLROnPlateau(patience = 10, factor = 0.25, verbose = 1)\r\n",
        "    cp = ModelCheckpoint(f'/content/drive/MyDrive/dacon/mcp/{alphabet}.hdf5')\r\n",
        "    model.compile(loss = 'binary_crossentropy', optimizer= 'adam', metrics = ['acc'])\r\n",
        "    history = model.fit(x_train, y_train, epochs = 10000, batch_size = 128, validation_split = 0.2, callbacks = [es,cp,lr])\r\n",
        "    val.append(history.history['val_loss'][-31])\r\n",
        "    acc.append(history.history['val_acc'][-31])\r\n",
        "\r\n",
        "print('loss : ', val)\r\n",
        "print('acc : ', acc)\r\n",
        "    #4. 평가 예측\r\n",
        "#     y_pred = model.predict(x_test)\r\n",
        "#     y_pred = np.where(y_pred<0.5,0,1)\r\n",
        "#     sub.loc[:,alphabet] = y_pred\r\n",
        "\r\n",
        "# sub.to_csv('/content/drive/MyDrive/dacon/submission_003.csv', index = 0)\r\n",
        "\r\n",
        "# loss :  [0.6772362589836121, 0.5581411719322205, 0.38123103976249695, 0.6617963910102844, 0.37605029344558716, 0.6572653651237488, 0.5549827814102173, 0.5124793648719788, 0.4877137839794159, 0.4501129388809204, 0.3214549124240875, 0.5551599860191345, 0.5238714218139648, 0.683418869972229, 0.3097933530807495, 0.4915141463279724, 0.620019257068634, 0.6751967072486877, 0.1978851854801178, 0.6415449380874634, 0.5357807874679565, 0.6829854249954224, 0.4044191539287567, 0.35578519105911255, 0.6477894186973572, 0.3420696556568146]\r\n",
        "# acc :  [0.569599986076355, 0.7466999888420105, 0.8467000126838684, 0.6310999989509583, 0.8379999995231628, 0.6625999808311462, 0.7145000100135803, 0.7595999836921692, 0.7706000208854675, 0.7932999730110168, 0.8715999722480774, 0.7429999709129333, 0.7480000257492065, 0.5593000054359436, 0.8701000213623047, 0.7818999886512756, 0.6916999816894531, 0.5810999870300293, 0.9259999990463257, 0.6399000287055969, 0.7461000084877014, 0.5634999871253967, 0.8270999789237976, 0.8568000197410583, 0.6462000012397766, 0.8622999787330627]"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}