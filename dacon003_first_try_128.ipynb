{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "dacon003_first_try_128.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "mount_file_id": "1dPtMl9zDkeH_5tU8kNtZgqJc4KkS6yBd",
      "authorship_tag": "ABX9TyO210pRJx9Vriu+wbCWPgDm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dongjaeseo/colab/blob/main/dacon003_first_try_128.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LGWQ94mP13Bp",
        "outputId": "4158c5f7-ff06-4ab7-b853-5c8b88daa4db"
      },
      "source": [
        "import numpy as np\r\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\r\n",
        "from tensorflow.keras.models import Model, Sequential\r\n",
        "from tensorflow.keras.layers import Dense, Conv2D, MaxPooling2D, Dropout, BatchNormalization, Flatten\r\n",
        "from sklearn.model_selection import train_test_split\r\n",
        "from tensorflow.keras.callbacks import ReduceLROnPlateau, EarlyStopping, ModelCheckpoint\r\n",
        "import string\r\n",
        "import pandas as pd\r\n",
        "import cv2\r\n",
        "\r\n",
        "def mymodel():\r\n",
        "    model = Sequential()\r\n",
        "\r\n",
        "    model.add(Conv2D(16,(3,3),activation='relu',input_shape=(128,128,1),padding='same'))\r\n",
        "    model.add(BatchNormalization())\r\n",
        "    model.add(Dropout(0.3))\r\n",
        "    \r\n",
        "    model.add(Conv2D(32,(3,3),activation='relu',padding='same'))\r\n",
        "    model.add(BatchNormalization())\r\n",
        "    model.add(Conv2D(32,(5,5),activation='relu',padding='same')) \r\n",
        "    model.add(BatchNormalization())\r\n",
        "    model.add(Conv2D(32,(5,5),activation='relu',padding='same'))\r\n",
        "    model.add(BatchNormalization())\r\n",
        "    model.add(Conv2D(32,(5,5),activation='relu',padding='same'))\r\n",
        "    model.add(BatchNormalization())\r\n",
        "    model.add(MaxPooling2D((3,3)))\r\n",
        "    model.add(Dropout(0.3))\r\n",
        "    \r\n",
        "    model.add(Conv2D(64,(3,3),activation='relu',padding='same'))\r\n",
        "    model.add(BatchNormalization())\r\n",
        "    model.add(Conv2D(64,(5,5),activation='relu',padding='same')) \r\n",
        "    model.add(BatchNormalization())\r\n",
        "    model.add(MaxPooling2D((3,3)))\r\n",
        "    model.add(Dropout(0.3))\r\n",
        "    \r\n",
        "    model.add(Flatten())\r\n",
        "\r\n",
        "    model.add(Dense(128,activation='relu'))\r\n",
        "    model.add(BatchNormalization())\r\n",
        "    model.add(Dropout(0.3))\r\n",
        "    model.add(Dense(64,activation='relu'))\r\n",
        "    model.add(BatchNormalization())\r\n",
        "    model.add(Dropout(0.3))\r\n",
        "\r\n",
        "    model.add(Dense(1,activation='sigmoid'))\r\n",
        "    return model\r\n",
        "    \r\n",
        "#1. 데이터\r\n",
        "x_train = np.load('/content/drive/MyDrive/dacon/npy/x_train_128.npy')\r\n",
        "x_test = np.load('/content/drive/MyDrive/dacon/npy/x_test_128.npy')\r\n",
        "answer = pd.read_csv('/content/drive/MyDrive/dacon/dirty_mnist_answer.csv')\r\n",
        "sub = pd.read_csv('/content/drive/MyDrive/dacon/sample_submission.csv', header = 0)\r\n",
        "\r\n",
        "# print(x_train.shape) (50000, 128, 128, 1)\r\n",
        "# print(np.max(x_train[0])) # 1.0\r\n",
        "\r\n",
        "alphabets = string.ascii_lowercase\r\n",
        "alphabets = list(alphabets)\r\n",
        "\r\n",
        "for alphabet in alphabets:\r\n",
        "    y_train = answer.loc[:,alphabet].to_numpy()\r\n",
        "\r\n",
        "    #2. 모델\r\n",
        "    model = mymodel()\r\n",
        "\r\n",
        "    #3. 컴파일 훈련\r\n",
        "    es = EarlyStopping(patience = 15)\r\n",
        "    lr = ReduceLROnPlateau(patience = 7, factor = 0.25, verbose = 1)\r\n",
        "    cp = ModelCheckpoint(f'/content/drive/MyDrive/dacon/mcp/{alphabet}.hdf5')\r\n",
        "    model.compile(loss = 'binary_crossentropy', optimizer= 'adam', metrics = ['acc'])\r\n",
        "    model.fit(x_train, y_train, epochs = 10000, batch_size = 128, validation_split = 0.2, callbacks = [es,cp,lr])\r\n",
        "\r\n",
        "    #4. 평가 예측\r\n",
        "    y_pred = model.predict(x_test)\r\n",
        "    y_pred = np.where(y_pred<0.5,0,1)\r\n",
        "    sub.loc[:,alphabet] = y_pred\r\n",
        "\r\n",
        "sub.to_csv('/content/drive/MyDrive/dacon/submission_003.csv', index = 0)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10000\n",
            "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_train_function.<locals>.train_function at 0x7efdde01a510> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
            "313/313 [==============================] - ETA: 0s - loss: 0.8162 - acc: 0.5148WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_test_function.<locals>.test_function at 0x7ef943f9d950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
            "313/313 [==============================] - 42s 127ms/step - loss: 0.8160 - acc: 0.5148 - val_loss: 0.9306 - val_acc: 0.5388\n",
            "Epoch 2/10000\n",
            "313/313 [==============================] - 39s 124ms/step - loss: 0.7001 - acc: 0.5467 - val_loss: 0.7019 - val_acc: 0.5536\n",
            "Epoch 3/10000\n",
            "230/313 [=====================>........] - ETA: 9s - loss: 0.6833 - acc: 0.5571"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}